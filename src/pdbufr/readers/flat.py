# # (C) Copyright 2019- ECMWF.
# #
# # This software is licensed under the terms of the Apache Licence Version 2.0
# # which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
# # In applying this licence, ECMWF does not waive the privileges and immunities
# # granted to it by virtue of its status as an intergovernmental organisation
# # nor does it submit to any jurisdiction.

# import collections
# from typing import Any
# from typing import Dict
# from typing import Iterable
# from typing import Iterator
# from typing import Mapping
# from typing import MutableMapping
# from typing import Sequence
# from typing import Set
# from typing import Union

# import eccodes  # type: ignore
# import numpy as np
# import pandas as pd  # type: ignore

# from pdbufr.core.filters import BufrFilter
# from pdbufr.core.filters import filters_match_header
# from pdbufr.core.keys import COMPUTED_KEYS
# from pdbufr.core.keys import UncompressedBufrKey
# from pdbufr.core.keys import RankedUncompressedBufrKey
# from pdbufr.core.keys import rank_from_key
# from pdbufr.core.structure import BufrHeader
# from pdbufr.core.structure import MessageWrapper

# from . import Reader

# SKIP_KEYS = {
#     "unexpandedDescriptors",
#     "shortDelayedDescriptorReplicationFactor",
#     "delayedDescriptorReplicationFactor",
#     "extendedDelayedDescriptorReplicationFactor",
#     "delayedDescriptorAndDataRepetitionFactor",
#     "extendedDelayedDescriptorAndDataRepetitionFactor" "associatedFieldSignificance",
#     "dataPresentIndicator",
#     "operator",
# }


# class ExtractConf:
#     def __init__(self, columns, filters, required_columns, header) -> None:
#         self.filters = filters
#         self.header_filters = {k: v for k, v in filters.items() if k in header}
#         self.data_filters = {k: v for k, v in filters.items() if k not in self.header_filters}
#         self.required_columns = required_columns
#         self.data_required_columns = required_columns - header.keys()

#     def match_header(self, message: Mapping[str, Any]) -> bool:
#         for key in self.header_filters:
#             name = key
#             value = message.get(key)
#             if isinstance(value, float) and value == eccodes.CODES_MISSING_DOUBLE:
#                 value = None
#             elif isinstance(value, int) and value == eccodes.CODES_MISSING_LONG:
#                 value = None
#             if not self.header_filters[name].match(value):
#                 return False


# def extract_blocks(
#     message: Mapping[str, Any],
#     header: BufrHeader,
#     add_header: bool,
#     add_data: bool,
#     data_filters: Dict[str, BufrFilter] = {},
#     data_required_columns: Set[str] = set(),
#     add_filters: bool = True,
# ) -> Iterator[Dict[str, Any]]:
#     value_cache = {}
#     try:
#         is_compressed = bool(message["compressedData"])
#     except KeyError:
#         is_compressed = False

#     if is_compressed:
#         is_uncompressed = False
#         subset_count = message["numberOfSubsets"]
#     else:
#         is_uncompressed = int(message["numberOfSubsets"]) > 1
#         subset_count = 1

#     # if add_data or data_filters or data_required_columns:
#     #     message["skipExtraKeyAttributes"] = 1
#     #     message["unpack"] = 1

#     if not is_uncompressed and not is_compressed:
#         yield from extract_blocks_standard(
#             message, header, add_header, add_data, data_filters, add_filters, data_required_columns
#         )
#     elif is_compressed:
#         yield from extract_blocks_compressed(
#             message,
#             header,
#             subset_count,
#             add_header,
#             add_data,
#             data_filters,
#             add_filters,
#             data_required_columns,
#         )
#     else:
#         # yield from extract_blocks_uncompressed(
#         #     message,
#         #     subset_count,
#         #     filters,
#         #     add_filters,
#         #     columns,
#         #     columns,
#         #     required_columns,
#         #     prefilter_headers,
#         #     header,
#         # )
#         pass

#     # For messages with uncompressed subsets consider this:
#     # - for each data key we have a single value
#     # - there is no way to identify the subset from the key
#     # - we cannot directly iterate over a given subset
#     # - if we iterate over the keys a new subset is indicated by the
#     #   appearance of the "subsetNumber" key, which contains the same array
#     #   of values each time (the subset index for all the subsets). This key is
#     #   generated by ecCodes and does not contain any ranking so its name is
#     #   always "subsetNumber".

#     uncompressed_keys = dict()
#     skip_keys = {
#         "unexpandedDescriptors",
#         "shortDelayedDescriptorReplicationFactor",
#         "delayedDescriptorReplicationFactor",
#         "extendedDelayedDescriptorReplicationFactor",
#         "delayedDescriptorAndDataRepetitionFactor",
#         "extendedDelayedDescriptorAndDataRepetitionFactor" "associatedFieldSignificance",
#         "dataPresentIndicator",
#         "operator",
#     }

#     for subset in range(subset_count):
#         filters_match = {k: False for k in filters.keys()}
#         required_columns_match = {k: False for k in required_columns}
#         current_observation: Dict[str, Any]
#         current_observation = collections.OrderedDict(base_observation)

#         uncompressed_subset = 0
#         for key in message:
#             name = key.rpartition("#")[2]
#             if name in skip_keys or "->" in key:
#                 continue

#             if is_uncompressed and key == "subsetNumber":
#                 if uncompressed_subset > 0:
#                     if (
#                         current_observation
#                         and all(filters_match.values())
#                         and all(required_columns_match.values())
#                     ):
#                         yield dict(current_observation)

#                     # header keys appear only once so we need to keep the match info for them
#                     for k in filters_match:
#                         if k not in header_keys:
#                             filters_match[k] = False

#                     for k in required_columns_match:
#                         if k not in header_keys:
#                             required_columns_match[k] = False

#                     # only keep the header keys in the result because they are the
#                     # same for all the subsets
#                     while len(current_observation) and "subsetNumber" in current_observation:
#                         # OrderedDict.popitem uses LIFO order
#                         current_observation.popitem()

#                     for v in uncompressed_keys.values():
#                         v.adjust_ref_rank()

#                 uncompressed_subset += 1

#             if is_compressed:
#                 if key not in value_cache:
#                     value_cache[key] = message.get(key)
#                 value = value_cache[key]
#             else:
#                 value = message.get(key)

#             # extract compressed BUFR values. They are either numpy arrays (for numeric types)
#             # or lists of strings
#             if (
#                 is_compressed
#                 and name != "unexpandedDescriptors"
#                 and isinstance(value, (np.ndarray, list))
#                 and len(value) == subset_count
#             ):
#                 value = value[subset]

#             if isinstance(value, float) and value == eccodes.CODES_MISSING_DOUBLE:
#                 value = None
#             elif isinstance(value, int) and value == eccodes.CODES_MISSING_LONG:
#                 value = None

#             if is_uncompressed:
#                 if name not in uncompressed_keys:
#                     uncompressed_keys[name] = UncompressedBufrKey.from_key(key)
#                 else:
#                     uncompressed_keys[name].update_rank(key)
#                 key = uncompressed_keys[name].relative_key

#                 # subsetNumber is an array and we need the current value
#                 if key == "subsetNumber":
#                     value = uncompressed_subset

#             if name in filters:
#                 if filters[name].match(value):
#                     filters_match[name] = True

#             if name in required_columns:
#                 required_columns_match[name] = True

#             current_observation[key] = value

#         # yield the last observation
#         if current_observation and all(filters_match.values()) and all(required_columns_match.values()):
#             yield dict(current_observation)


# def extract_blocks_standard(
#     message,
#     header,
#     add_header,
#     add_data,
#     data_filters,
#     add_filters,
#     data_required_columns,
# ):
#     result = dict()

#     if not header.match_filters():
#         return

#     if add_header:
#         result = header.values()
#     elif add_filters:
#         result = header.filters_values()

#     if add_data or data_filters or data_required_columns:
#         data_required_columns_match_count = 0

#         message["skipExtraKeyAttributes"] = 1
#         message["unpack"] = 1

#         def _get_value(key):
#             value = message.get(key)
#             if isinstance(value, float) and value == eccodes.CODES_MISSING_DOUBLE:
#                 value = None
#             elif isinstance(value, int) and value == eccodes.CODES_MISSING_LONG:
#                 value = None
#             return value

#         # first check the filters and required columns
#         columns = list(data_filters.keys()) + list(data_required_columns)
#         for key in columns:

#             value = _get_value(key)

#             if key in data_filters:
#                 if not data_filters[name].match(value):
#                     return

#             if name in data_required_columns:
#                 data_required_columns_match_count += 1

#             if not add_data and add_filters:
#                 result[key] = value

#         if data_required_columns_match_count != len(data_required_columns):
#             return

#         # extract all the data keys
#         if add_data:
#             in_data = False
#             for key in message:
#                 if not in_data and key == header.last_key:
#                     in_data = True
#                     continue

#                 name = key.rpartition("#")[2]
#                 if name in SKIP_KEYS or "->" in key:
#                     continue

#                 value = _get_value(key)
#                 result[key] = value

#     # yield the result
#     if result:
#         # print("yielding:", dict(result))
#         yield dict(result)


# def extract_blocks_compressed(
#     message, header, subset_count, add_header, add_data, data_filters, add_filters, data_required_columns
# ):
#     value_cache = {}

#     if not header.match_filters():
#         return

#     if add_header:
#         result = header.values()
#     elif add_filters:
#         result = header.filters_values()

#     if add_data or data_filters or data_required_columns:
#         data_required_columns_match_count = 0

#         message["skipExtraKeyAttributes"] = 1
#         message["unpack"] = 1

#         def _get_value(key):
#             if key not in value_cache:
#                 value_cache[key] = message.get(key)
#             value = value_cache[key]

#             # extract compressed BUFR values. They are either numpy arrays (for numeric types)
#             # or lists of strings
#             if (
#                 key != "unexpandedDescriptors"
#                 and isinstance(value, (np.ndarray, list))
#                 and len(value) == subset_count
#             ):
#                 value = value[subset]

#             if isinstance(value, float) and value == eccodes.CODES_MISSING_DOUBLE:
#                 value = None
#             elif isinstance(value, int) and value == eccodes.CODES_MISSING_LONG:
#                 value = None

#         columns = list(data_filters.keys()) + list(data_required_columns)
#         for subset in range(subset_count):
#             current_result = dict(result)
#             # first check the filters and required columns
#             for key in columns:
#                 value = _get_value(key)

#                 if key in data_filters:
#                     if not data_filters[name].match(value):
#                         return

#                 if name in data_required_columns:
#                     data_required_columns_match_count += 1

#                 if not add_data and add_filters:
#                     current_result[key] = value

#             if data_required_columns_match_count != len(data_required_columns):
#                 continue

#             # extract all the data keys
#             if add_data:
#                 in_data = False
#                 for key in message:
#                     if not in_data and key == header.last_key:
#                         in_data = True
#                         continue

#                     name = key.rpartition("#")[2]
#                     if name in SKIP_KEYS or "->" in key:
#                         continue

#                     value = _get_value(key)
#                     current_result[key] = value

#                 # yield the result
#                 if current_result:
#                     # print("yielding:", dict(current_observation))
#                     yield dict(current_result)

#     elif result:
#         yield dict(result)


# def extract_keys(
#     message: Mapping[str, Any],
#     filters: Dict[str, BufrFilter] = {},
#     add_filters: bool = True,
#     columns: Set[str] = set(),
#     required_columns: Set[str] = set(),
#     prefilter_headers: bool = False,
#     header: BufrHeader = BufrHeader({}),
# ) -> Iterator[Dict[str, Any]]:
#     try:
#         is_compressed = bool(message["compressedData"])
#     except KeyError:
#         is_compressed = False

#     if is_compressed:
#         is_uncompressed = False
#         subset_count = message["numberOfSubsets"]
#     else:
#         is_uncompressed = int(message["numberOfSubsets"]) > 1
#         subset_count = 1

#     # For messages with uncompressed subsets consider this:
#     # - for each data key we have a single value
#     # - there is no way to identify the subset from the key
#     # - we cannot directly iterate over a given subset
#     # - if we iterate over the keys a new subset is indicated by the
#     #   appearance of the "subsetNumber" key, which contains the same array
#     #   of values each time (the subset index for all the subsets). This key is
#     #   generated by ecCodes and does not contain any ranking so its name is
#     #   always "subsetNumber".

#     uncompressed_keys: Dict["str", Any]
#     uncompressed_keys = dict()
#     skip_keys = {
#         "unexpandedDescriptors",
#         "shortDelayedDescriptorReplicationFactor",
#         "delayedDescriptorReplicationFactor",
#         "extendedDelayedDescriptorReplicationFactor",
#         "delayedDescriptorAndDataRepetitionFactor",
#         "extendedDelayedDescriptorAndDataRepetitionFactor" "associatedFieldSignificance",
#         "dataPresentIndicator",
#         "operator",
#     }

#     # print("columns:", columns)
#     # print("filters:", filters)

#     if not is_uncompressed and not is_compressed:
#         yield from extract_keys_standard(message, filters, add_filters, columns, required_columns)
#     elif is_compressed:
#         yield from extract_keys_compressed(
#             message, subset_count, filters, add_filters, columns, required_columns, prefilter_headers, header
#         )
#     else:
#         yield from extract_keys_uncompressed(
#             message,
#             subset_count,
#             filters,
#             add_filters,
#             columns,
#             columns,
#             required_columns,
#             prefilter_headers,
#             header,
#         )


# def extract_keys_standard(message, filters, add_filters, columns, required_columns):
#     required_columns_match_count = 0
#     result = dict()

#     # filter keys should be extracted first
#     all_columns = columns
#     if filters:
#         all_columns = list(filters.keys())
#         for k in columns:
#             if k not in filters:
#                 all_columns.append(k)

#     # filter keys are at the start of all_columns
#     for key in all_columns:
#         name = key

#         value = message.get(key)

#         if isinstance(value, float) and value == eccodes.CODES_MISSING_DOUBLE:
#             value = None
#         elif isinstance(value, int) and value == eccodes.CODES_MISSING_LONG:
#             value = None

#         if name in filters:
#             if not filters[name].match(value):
#                 continue

#         if name in required_columns:
#             required_columns_match_count += 1

#         if add_filters or key in columns:
#             result[key] = value

#     # yield the result
#     if result and required_columns_match_count == len(required_columns):
#         # print("yielding:", dict(current_observation))
#         yield dict(result)


# def extract_keys_compressed(
#     message, subset_count, filters, add_filters, columns, required_columns, prefilter_headers, header
# ):
#     value_cache = {}

#     # check header filters in advance
#     if not prefilter_headers and filters:
#         header_filters = {k: v for k, v in filters.items() if k in header}
#         data_filters = {k: v for k, v in filters.items() if k not in header}

#         for key in header_filters:
#             name = key

#             if key not in value_cache:
#                 value_cache[key] = message.get(key)
#             value = value_cache[key]

#             if not header_filters[name].match(value):
#                 return

#         filters = data_filters

#     # filter keys should be extracted first
#     all_columns = columns
#     if filters:
#         all_columns = list(filters.keys())
#         for k in columns:
#             if k not in filters:
#                 all_columns.append(k)

#     for subset in range(subset_count):
#         required_columns_match_count = 0
#         result = dict()

#         for key in all_columns:
#             name = key

#             if key not in value_cache:
#                 value_cache[key] = message.get(key)
#             value = value_cache[key]

#             # extract compressed BUFR values. They are either numpy arrays (for numeric types)
#             # or lists of strings
#             if (
#                 key != "unexpandedDescriptors"
#                 and isinstance(value, (np.ndarray, list))
#                 and len(value) == subset_count
#             ):
#                 value = value[subset]

#             if isinstance(value, float) and value == eccodes.CODES_MISSING_DOUBLE:
#                 value = None
#             elif isinstance(value, int) and value == eccodes.CODES_MISSING_LONG:
#                 value = None

#             if name in filters:
#                 if not filters[name].match(value):
#                     continue

#             if name in required_columns:
#                 required_columns_match_count += 1

#             if add_filters or key in columns:
#                 result[key] = value

#         # yield the result
#         if result and required_columns_match_count == len(required_columns):
#             # print("yielding:", dict(current_observation))
#             yield dict(result)


# def extract_keys_uncompressed(
#     message, subset_count, filters, add_filters, columns, required_columns, prefilter_headers, header
# ):
#     # For messages with uncompressed subsets consider this:
#     # - for each data key we have a single value
#     # - there is no way to identify the subset from the key
#     # - we cannot directly iterate over a given subset
#     # - if we iterate over the keys a new subset is indicated by the
#     #   appearance of the "subsetNumber" key, which contains the same array
#     #   of values each time (the subset index for all the subsets). This key is
#     #   generated by ecCodes and does not contain any ranking so its name is
#     #   always "subsetNumber".

#     header_cache = {}
#     header_result = {}

#     # check header filters in advance
#     if not prefilter_headers and filters:
#         header_filters = {k: v for k, v in filters.items() if k in header}
#         data_filters = {k: v for k, v in filters.items() if k not in header}

#         for key in header_filters:
#             name = key

#             if key not in header_cache:
#                 header_cache[key] = message.get(key)
#             value = header_cache[key]

#             if not header_filters[name].match(value):
#                 return

#         filters = data_filters

#     header_columns = [k for k in columns if k in header]
#     for key in header_columns:
#         if key not in header_cache:
#             header_cache[key] = message.get(key)
#         header_result[key] = header_cache[key]

#     data_columns = ["subsetNumber"] + [k for k in columns if k not in header_columns]

#     skip_keys = {
#         "unexpandedDescriptors",
#         "shortDelayedDescriptorReplicationFactor",
#         "delayedDescriptorReplicationFactor",
#         "extendedDelayedDescriptorReplicationFactor",
#         "delayedDescriptorAndDataRepetitionFactor",
#         "extendedDelayedDescriptorAndDataRepetitionFactor" "associatedFieldSignificance",
#         "dataPresentIndicator",
#         "operator",
#     }

#     # filter keys should be extracted first
#     all_columns = columns
#     if filters:
#         all_columns = list(filters.keys())
#         for k in columns:
#             if k not in filters:
#                 all_columns.append(k)

#     subset = 0
#     rank_offset = {}
#     for key in all_columns:
#         name = key.rpartition("#")[2]
#         rank_offset[name] = 0

#     # ranked_keys = {RankedUncompressedBufrKey(k) for k in all_columns}

#     required_columns_match_count = 0

#     for key in message:
#         # start new subset
#         if key == "subsetNumber":
#             all_columns.insert(0, key)
#             if result and required_columns_match_count == len(required_columns):
#                 yield dict(result)

#             subset += 1
#             required_columns_match_count = 0
#             rank_offset = {k: 0 for k in rank_offset}
#         else:
#             name = key.rpartition("#")[2]
#             if name in skip_keys or "->" in key:
#                 continue

#             if name in rank_offset:
#                 if rank_offset[name] == 0:
#                     rank = rank_from_key(key)
#                     rank_offset[name] = rank
#                 key = rerank(key, rank_offset[name])

#                 value = message.get(key)
#                 if isinstance(value, float) and value == eccodes.CODES_MISSING_DOUBLE:
#                     value = None
#                 elif isinstance(value, int) and value == eccodes.CODES_MISSING_LONG:
#                     value = None

#                 if name in filters:
#                     if not filters[name].match(value):
#                         continue

#                 if name in required_columns:
#                     required_columns_match += 1

#                 if add_filters or key in columns:
#                     result[key] = value

#         # handle last subset
#         if result and filters_match == len(filters) and required_columns_match == len(required_columns):
#             # print("yielding:", dict(current_observation))
#             yield dict(result)


# def test_computed_keys(
#     observation: Dict[str, Any],
#     filters: Dict[str, BufrFilter] = {},
#     prefix: str = "",
# ) -> bool:
#     # print("testing computed keys with filters:", filters, prefix)
#     for keys, computed_key, getter in COMPUTED_KEYS:
#         if computed_key in filters:
#             computed_value = None
#             try:
#                 computed_value = getter(observation, prefix, keys)
#             except Exception:
#                 return False
#             if computed_value is not None:
#                 if not filters[computed_key].match(computed_value):
#                     return False
#             else:
#                 return False
#     return True


# class FlatReader(Reader):
#     class ColumnInfo:
#         def __init__(self) -> None:
#             self.first_count = 0

#     def __init__(
#         self,
#         path_or_messages,
#         columns: Union[Sequence[str], str] = [],
#         **kwargs: Any,
#     ):
#         class ColumnInfo:
#             def __init__(self) -> None:
#                 self.first_count = 0

#         self.column_info = ColumnInfo()
#         super().__init__(path_or_messages, columns=columns, column_info=self.column_info, **kwargs)

#     def execute(
#         self,
#         # columns: Union[Sequence[str], str] = [],
#         # filters: Mapping[str, Any] = {},
#         # required_columns: Union[bool, Iterable[str]] = True,
#         # **kwargs,
#     ) -> pd.DataFrame:
#         # class ColumnInfo:
#         #     def __init__(self) -> None:
#         #         self.first_count = 0

#         # column_info = ColumnInfo()
#         df = super().execute(
#             # columns=columns,
#             # filters=filters,
#             # required_columns=required_columns,
#             # column_info=column_info,
#             # **kwargs,
#         )

#         # compare the column count in the first record to that of the
#         # dataframe. If the latter is larger, then there were non-aligned columns,
#         # which were appended to the end of the dataframe columns.
#         if self.column_info.first_count > 0 and self.column_info.first_count < len(df.columns):
#             import warnings

#             # temporarily overwrite warnings formatter
#             ori_formatwarning = warnings.formatwarning
#             warnings.formatwarning = lambda msg, *args, **kwargs: f"Warning: {msg}\n"
#             warnings.warn(
#                 (
#                     "not all BUFR messages/subsets have the same structure in the input file. "
#                     "Non-overlapping columns (starting with column[{column_info.first_count-1}] ="
#                     f"{df.columns[self.column_info.first_count-1]}) were added to end of the resulting dataframe"
#                     "altering the original column order for these messages."
#                 )
#             )
#             warnings.formatwarning = ori_formatwarning

#         return df

#     def read_records(
#         self,
#         bufr_obj: Iterable[MutableMapping[str, Any]],
#         columns: Union[Sequence[str], str],
#         filters: Mapping[str, Any] = {},
#         required_columns: Union[bool, Iterable[str]] = True,
#         prefilter_headers: bool = False,
#         column_info: Any = None,
#     ) -> Iterator[Dict[str, Any]]:
#         if isinstance(columns, str):
#             columns = (columns,)

#         if not isinstance(columns, Sequence):
#             raise TypeError("invalid columns type")
#         elif len(columns) == 0 or columns[0] == "":
#             columns = ["all"]
#         # elif len(columns) != 1:
#         #     raise ValueError("when columns is an iterable it can have maximum 1 element")

#         # if columns[0] not in ["all", "header", "data"]:
#         #     raise ValueError("columns must be all, header or data")

#         assert len(columns) > 0

#         add_header = False
#         add_data = False

#         add_filters = True

#         if "all" in columns:
#             if len(columns) > 1:
#                 raise ValueError("when 'all' is specified no other columns can be specified")
#         elif "header" in columns and "data" in columns:
#             if len(columns) > 2:
#                 raise ValueError(
#                     "when both 'header' and 'data' is specified no other columns can be specified"
#                 )

#         if "all" in columns or "header" in columns:
#             add_header = True

#         if "all" in columns or "data" in columns:
#             add_data = True

#         block_keys = ("all", "header", "data")
#         key_columns = [col for col in columns if col not in block_keys]

#         # if any(col in ["all", "header", "data"] for col in columns):
#         #     raise ValueError("columns must be all, header or data")

#         # add_header = columns[0] in ["all", "header"]
#         # add_data = columns[0] in ["all", "data"]
#         # if not add_header and not add_data:
#         #     raise ValueError("either header or data must be extracted")

#         if isinstance(required_columns, bool):
#             required_columns = set()
#         elif isinstance(required_columns, str):
#             required_columns = {required_columns}
#         elif isinstance(required_columns, Iterable):
#             required_columns = set(required_columns)
#         else:
#             raise TypeError("required_columns must be a bool, str or an iterable")

#         # compile filters
#         filters = dict(filters)
#         filters = {k: BufrFilter.from_user(filters[k], key=k) for k in filters}

#         # prepare count filter
#         if "count" in filters:
#             max_count = filters["count"].max()
#         else:
#             max_count = None

#         count_filter = filters.pop("count", None)

#         # prepare computed keys
#         computed_keys = [x for _, x, _ in COMPUTED_KEYS]
#         column_filters = {k: v for k, v in filters.items() if k not in computed_keys}
#         computed_filters = {k: v for k, v in filters.items() if k in computed_keys}

#         if column_info is not None:
#             column_info.first_count = 0

#         for count, msg in enumerate(bufr_obj, 1):
#             # We use a context manager to automatically delete the handle of the BufrMessage.
#             # We have to use a wrapper object here because a message can also be a dict
#             with MessageWrapper.wrap_context(msg) as message:
#                 # count filter
#                 if count_filter is not None and not count_filter.match(count):
#                     continue

#                 # message_value_filters = value_filters_without_computed
#                 # message_required_columns = required_columns
#                 header = BufrHeader(message)
#                 data_filters = {k: v for k, v in column_filters.items() if k not in header.filters}

#                 # test filters on header keys before unpacking
#                 if prefilter_headers:
#                     if not header.filters_match():
#                         continue

#                 data_required_columns = required_columns
#                 if data_required_columns:
#                     data_required_columns = data_required_columns - header.keys

#                 # get full header or data sections
#                 if add_header or add_data:
#                     for record in extract_blocks(
#                         message,
#                         header,
#                         add_header,
#                         add_data,
#                         data_filters,
#                         add_filters,
#                         data_required_columns,
#                     ):
#                         if record and test_computed_keys(record, computed_filters, "#1#"):
#                             if column_info is not None and column_info.first_count == 0:
#                                 column_info.first_count = len(record)
#                             yield record

#                 # get list of specific keys
#                 else:
#                     for record in extract_keys(
#                         message,
#                         header,
#                         add_header,
#                         add_data,
#                         data_filters,
#                         add_filters,
#                         data_required_columns,
#                     ):
#                         if record and test_computed_keys(record, computed_filters, "#1#"):
#                             yield record


#             # optimisation: skip decoding messages above max_count
#             if max_count is not None and count >= max_count:
#                 break

#     def adjust_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
#         return df


# reader = FlatReader
